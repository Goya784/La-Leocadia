---
title: "Bioinformatics Class 8"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means clusetering example

Lets make up some data for testing the 'kmeans()' function

```{r}
# Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```



use the kmeans() function setting k to 2 and nstart= 20
```{r}
km <- kmeans(x, centers =2, nstart =20)
km
```

Inspect/print results

Q. How many points are in each cluster?

```{r}
km$size
```

What 'component' of your result object details
  -cluster size
  -cluster assignment/membership
  -cluster center?
  
```{r}
km$cluster
```

Cluster center

```{r}
km$centers
```

Plot x colored by the kmeans cluster assigmnent and add cluster centers as blue points

```{r}
plot(x, col=km$cluster, pch =16)
points(km$centers, col="blue" )
```

# Hierarchical clustering

First we need to calculate point (dis)similarity as the Euclidean distance between observations

```{r}
dist_matrix <- dist(x)
```

Convert to matrix to see the structure of this distance matrix and find the dimensions


```{r}
dim(as.matrix(dist_matrix))
```

The hclust() function returns a hierarchical
```{r}
hc <- hclust(dist(x))
```

```{r}
# Draw a dendogram
plot(hc)
abline(h=6, col= "red")
cutree(hc, h=6) # Cut by the height h
```


Let's 'cut' our tree to define clusters

```{r}
grps <- cutree(hc, h=6)
table(grps)
```

Try different cutting

```{r}
plot(x, col=cutree(hc, k=4))
```



```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
  matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2),   # c1
  matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
  matrix(c(rnorm(50, mean = 1, sd = 0.3),           # c3
           rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")

x

```


```{r}
# Step 2. Plot the data without clustering
plot(x)
```



```{r}
# Step 3. Generate colors for known clusters 
#         (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) ) 
plot(x, col=col)
```

### Let's try hclust() on this data

```{r}
d <- dist(x)
hc <- hclust(d)
plot(hc)
```


Let's cut into 2 groups

```{r}
grps <- cutree(hc, k=3)
table(grps)
```

Plot the data colored by the cluster

```{r}
plot(x, col=grps)
```


# Principal Component Analysis (PCA)

```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV", row.names=1)

## lets do PCA 
pca <- prcomp(t(mydata), scale=TRUE)

## See what is returned by the prcomp() function
attributes(pca)

## A basic PC1 vs PC2 2-D plot 
plot(pca$x[,1], pca$x[,2]) 

## Variance captured per PC is often more informative to look at
pca.var <- pca$sdev^2 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1) 
pca.var.per

barplot(pca.var.per, main="Scree Plot",  xlab="Principal Component", ylab="Percent Variation")

## A vector of colors for wt and ko samples 
colvec <- as.factor( substr( colnames(mydata), 1, 2) ) 
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16, 
     xlab=paste0("PC1 (", pca.var.per[1], "%)"), 
     ylab=paste0("PC2 (", pca.var.per[2], "%)")) 
```


color up our PCA plot
```{r}
colnames(mydata)
```



```{r}
substr(colnames(mydata), 1 ,2)
```

```{r}
as.factor( substr(colnames(mydata),1,2))
```

```{r}
my_cols <- as.factor( substr(colnames(mydata),1,2))
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16, 
     xlab=paste0("PC1 (", pca.var.per[1], "%)"), 
     ylab=paste0("PC2 (", pca.var.per[2], "%)")) 
```


```{r}
 #PC91 (91%)
paste("PC1 (", pca.var.per[1], "%)", sep="")

```






